{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiShivam/Pycaret/blob/master/Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq4aMookzPhu"
      },
      "source": [
        "import torch\r\n",
        "# The autograd package provides automatic differentiation \r\n",
        "# for all operations on Tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rfoCAIlzXb7"
      },
      "source": [
        "# requires_grad = True -> tracks all operations on the tensor. \r\n",
        "x = torch.randn(3, requires_grad=True)\r\n",
        "y = x + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UOA30VlzZ0o",
        "outputId": "ec13a66d-ae78-4208-8a0c-3b34c4f6b9ac"
      },
      "source": [
        "# y was created as a result of an operation, so it has a grad_fn attribute.\r\n",
        "# grad_fn: references a Function that has created the Tensor\r\n",
        "print(x) # created by the user -> grad_fn is None\r\n",
        "print(y)\r\n",
        "print(y.grad_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.9354, 1.8400, 0.4647], requires_grad=True)\n",
            "tensor([2.9354, 3.8400, 2.4647], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7fdebb83bb50>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgvitzbCzb8S",
        "outputId": "7e53974b-5352-421b-8049-a55cfd6c624c"
      },
      "source": [
        "# Do more operations on y\r\n",
        "z = y * y * 3\r\n",
        "print(z)\r\n",
        "z = z.mean()\r\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([25.8501, 44.2366, 18.2239], grad_fn=<MulBackward0>)\n",
            "tensor(29.4368, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMWK-ohxzeVq",
        "outputId": "b0c490ad-047c-450f-8be8-8634a442f68c"
      },
      "source": [
        "# Let's compute the gradients with backpropagation\r\n",
        "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\r\n",
        "# The gradient for this tensor will be accumulated into .grad attribute.\r\n",
        "# It is the partial derivate of the function w.r.t. the tensor\r\n",
        "\r\n",
        "z.backward()\r\n",
        "print(x.grad) # dz/dx\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.8708, 7.6800, 4.9293])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTt3sfjazg94"
      },
      "source": [
        "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\r\n",
        "# It computes partial derivates while applying the chain rule\r\n",
        "\r\n",
        "# -------------\r\n",
        "# Model with non-scalar output:\r\n",
        "# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \r\n",
        "# specify a gradient argument that is a tensor of matching shape.\r\n",
        "# needed for vector-Jacobian product\r\n",
        "\r\n",
        "x = torch.randn(3, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfryC0rnzpNj",
        "outputId": "275c2028-57b9-4863-982c-2b3249d5d241"
      },
      "source": [
        "y = x * 2\r\n",
        "for _ in range(10):\r\n",
        "    y = y * 2\r\n",
        "\r\n",
        "print(y)\r\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1087.4006,  1971.4565,  4006.4727], grad_fn=<MulBackward0>)\n",
            "torch.Size([3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRbT1Rl5zsQd",
        "outputId": "f291b7cd-f0f1-4c5a-ca01-8700da8c2b67"
      },
      "source": [
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\r\n",
        "y.backward(v)\r\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzJQHPETzvO7",
        "outputId": "5da99ab5-cd82-4408-871a-ddf848ad2cbd"
      },
      "source": [
        "# -------------\r\n",
        "# Stop a tensor from tracking history:\r\n",
        "# For example during our training loop when we want to update our weights\r\n",
        "# then this update operation should not be part of the gradient computation\r\n",
        "# - x.requires_grad_(False)\r\n",
        "# - x.detach()\r\n",
        "# - wrap in 'with torch.no_grad():'\r\n",
        "\r\n",
        "# .requires_grad_(...) changes an existing flag in-place.\r\n",
        "a = torch.randn(2, 2)\r\n",
        "print(a.requires_grad)\r\n",
        "b = ((a * 3) / (a - 1))\r\n",
        "print(b.grad_fn)\r\n",
        "a.requires_grad_(True)\r\n",
        "print(a.requires_grad)\r\n",
        "b = (a * a).sum()\r\n",
        "print(b.grad_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "None\n",
            "True\n",
            "<SumBackward0 object at 0x7fde6e666910>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6nmvxm1zy4r",
        "outputId": "26a66d96-a8a7-43c9-d936-ee076548afa7"
      },
      "source": [
        "\r\n",
        "# .detach(): get a new Tensor with the same content but no gradient computation:\r\n",
        "a = torch.randn(2, 2, requires_grad=True)\r\n",
        "print(a.requires_grad)\r\n",
        "b = a.detach()\r\n",
        "print(b.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkAQq2pmz2Ch",
        "outputId": "5c023235-fa28-41d4-e5cb-d2c82f1b3053"
      },
      "source": [
        "# wrap in 'with torch.no_grad():'\r\n",
        "a = torch.randn(2, 2, requires_grad=True)\r\n",
        "print(a.requires_grad)\r\n",
        "with torch.no_grad():\r\n",
        "    print((x ** 2).requires_grad)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr6DPOa4z40K"
      },
      "source": [
        "# -------------\r\n",
        "# backward() accumulates the gradient for this tensor into .grad attribute.\r\n",
        "# !!! We need to be careful during optimization !!!\r\n",
        "# Use .zero_() to empty the gradients before a new optimization step!\r\n",
        "weights = torch.ones(4, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur0jkLD1zj2K",
        "outputId": "a0dfcc72-9fb9-41db-e1de-0b04d1df045a"
      },
      "source": [
        "\r\n",
        "for epoch in range(3):\r\n",
        "    # just a dummy example\r\n",
        "    model_output = (weights*3).sum()\r\n",
        "    model_output.backward()\r\n",
        "    \r\n",
        "    print(weights.grad)\r\n",
        "\r\n",
        "    # optimize model, i.e. adjust weights...\r\n",
        "    with torch.no_grad():\r\n",
        "        weights -= 0.1 * weights.grad\r\n",
        "\r\n",
        "    # this is important! It affects the final weights & output\r\n",
        "    weights.grad.zero_()\r\n",
        "\r\n",
        "print(weights)\r\n",
        "print(model_output)\r\n",
        "\r\n",
        "# Optimizer has zero_grad() method\r\n",
        "# optimizer = torch.optim.SGD([weights], lr=0.1)\r\n",
        "# During training:\r\n",
        "# optimizer.step()\r\n",
        "# optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
            "tensor(4.8000, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}